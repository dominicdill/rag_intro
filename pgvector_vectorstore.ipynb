{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8978177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_docling.loader import ExportType\n",
    "\n",
    "load_dotenv()\n",
    "EMBED_MODEL_ID = os.getenv(\"MODEL_ID\")\n",
    "EXPORT_TYPE = ExportType.DOC_CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dee90661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- d.page_content='Docling Technical Report\\nVersion 1.0\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\nAI4K Group, IBM Research R¨ uschlikon, Switzerland'\n",
      "- d.page_content='Abstract\\nThis technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.'\n",
      "- d.page_content='1 Introduction\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\nHere is what Docling delivers today:\\n· Converts PDF documents to JSON or Markdown format, stable and lightning fast\\n· Understands detailed page layout, reading order, locates figures and recovers table structures\\n· Extracts metadata from the document, such as title, authors, references and language\\n· Optionally applies OCR, e.g. for scanned PDFs\\n· Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\n· Can leverage different accelerators (GPU, MPS, etc).'\n"
     ]
    }
   ],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "FILE_PATH = \"https://arxiv.org/pdf/2408.09869\"\n",
    "\n",
    "loader = DoclingLoader(file_path=FILE_PATH)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "for d in docs[:3]:\n",
    "    print(f\"- {d.page_content=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f775119c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://arxiv.org/pdf/2408.09869',\n",
       " 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta',\n",
       "  'version': '1.0.0',\n",
       "  'doc_items': [{'self_ref': '#/texts/8',\n",
       "    'parent': {'$ref': '#/body'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'text',\n",
       "    'prov': [{'page_no': 1,\n",
       "      'bbox': {'l': 108.0,\n",
       "       't': 239.37,\n",
       "       'r': 504.003,\n",
       "       'b': 143.54600000000005,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 792]}]},\n",
       "   {'self_ref': '#/texts/9',\n",
       "    'parent': {'$ref': '#/body'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'text',\n",
       "    'prov': [{'page_no': 1,\n",
       "      'bbox': {'l': 108.0,\n",
       "       't': 135.88800000000003,\n",
       "       'r': 504.003,\n",
       "       'b': 83.52099999999996,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 488]}]},\n",
       "   {'self_ref': '#/texts/12',\n",
       "    'parent': {'$ref': '#/body'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'text',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 108.0,\n",
       "       't': 716.523,\n",
       "       'r': 253.972,\n",
       "       'b': 707.971,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 36]}]},\n",
       "   {'self_ref': '#/texts/13',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 695.23,\n",
       "       'r': 468.397,\n",
       "       'b': 686.678,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 78]}]},\n",
       "   {'self_ref': '#/texts/14',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 680.366,\n",
       "       'r': 504.003,\n",
       "       'b': 660.905,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 96]}]},\n",
       "   {'self_ref': '#/texts/15',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 654.593,\n",
       "       'r': 480.85,\n",
       "       'b': 646.041,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 86]}]},\n",
       "   {'self_ref': '#/texts/16',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 639.729,\n",
       "       'r': 333.463,\n",
       "       'b': 631.177,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 47]}]},\n",
       "   {'self_ref': '#/texts/17',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 624.866,\n",
       "       'r': 504.003,\n",
       "       'b': 605.405,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 161]}]},\n",
       "   {'self_ref': '#/texts/18',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 599.093,\n",
       "       'r': 355.411,\n",
       "       'b': 590.541,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 54]}]}],\n",
       "  'headings': ['1 Introduction'],\n",
       "  'origin': {'mimetype': 'application/pdf',\n",
       "   'binary_hash': 11465328351749295394,\n",
       "   'filename': '2408.09869v5.pdf'}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f87d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['schema_name', 'version', 'doc_items', 'headings', 'origin'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.metadata['dl_meta'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1185aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (612 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- d.page_content='1 Introduction\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\nHere is what Docling delivers today:\\n· Converts PDF documents to JSON or Markdown format, stable and lightning fast\\n· Understands detailed page layout, reading order, locates figures and recovers table structures\\n· Extracts metadata from the document, such as title, authors, references and language\\n· Optionally applies OCR, e.g. for scanned PDFs\\n· Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\n· Can leverage different accelerators (GPU, MPS, etc).'\n",
      "- d.page_content='1 Introduction\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\nHere is what Docling delivers today:\\n· Converts PDF documents to JSON or Markdown format, stable and lightning fast\\n· Understands detailed page layout, reading order, locates figures and recovers table structures\\n· Extracts metadata from the document, such as title, authors, references and language\\n· Optionally applies OCR, e.g. for scanned PDFs\\n· Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\n· Can leverage different accelerators (GPU, MPS, etc).'\n",
      "- d.page_content='1 Introduction\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\nHere is what Docling delivers today:\\n· Converts PDF documents to JSON or Markdown format, stable and lightning fast\\n· Understands detailed page layout, reading order, locates figures and recovers table structures\\n· Extracts metadata from the document, such as title, authors, references and language\\n· Optionally applies OCR, e.g. for scanned PDFs\\n· Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\n· Can leverage different accelerators (GPU, MPS, etc).'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': 'https://arxiv.org/pdf/2408.09869',\n",
       " 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta',\n",
       "  'version': '1.0.0',\n",
       "  'doc_items': [{'self_ref': '#/texts/8',\n",
       "    'parent': {'$ref': '#/body'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'text',\n",
       "    'prov': [{'page_no': 1,\n",
       "      'bbox': {'l': 108.0,\n",
       "       't': 239.37,\n",
       "       'r': 504.003,\n",
       "       'b': 143.54600000000005,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 792]}]},\n",
       "   {'self_ref': '#/texts/9',\n",
       "    'parent': {'$ref': '#/body'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'text',\n",
       "    'prov': [{'page_no': 1,\n",
       "      'bbox': {'l': 108.0,\n",
       "       't': 135.88800000000003,\n",
       "       'r': 504.003,\n",
       "       'b': 83.52099999999996,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 488]}]},\n",
       "   {'self_ref': '#/texts/12',\n",
       "    'parent': {'$ref': '#/body'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'text',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 108.0,\n",
       "       't': 716.523,\n",
       "       'r': 253.972,\n",
       "       'b': 707.971,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 36]}]},\n",
       "   {'self_ref': '#/texts/13',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 695.23,\n",
       "       'r': 468.397,\n",
       "       'b': 686.678,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 78]}]},\n",
       "   {'self_ref': '#/texts/14',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 680.366,\n",
       "       'r': 504.003,\n",
       "       'b': 660.905,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 96]}]},\n",
       "   {'self_ref': '#/texts/15',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 654.593,\n",
       "       'r': 480.85,\n",
       "       'b': 646.041,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 86]}]},\n",
       "   {'self_ref': '#/texts/16',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 639.729,\n",
       "       'r': 333.463,\n",
       "       'b': 631.177,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 47]}]},\n",
       "   {'self_ref': '#/texts/17',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 624.866,\n",
       "       'r': 504.003,\n",
       "       'b': 605.405,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 161]}]},\n",
       "   {'self_ref': '#/texts/18',\n",
       "    'parent': {'$ref': '#/groups/2'},\n",
       "    'children': [],\n",
       "    'content_layer': 'body',\n",
       "    'label': 'list_item',\n",
       "    'prov': [{'page_no': 2,\n",
       "      'bbox': {'l': 135.397,\n",
       "       't': 599.093,\n",
       "       'r': 355.411,\n",
       "       'b': 590.541,\n",
       "       'coord_origin': 'BOTTOMLEFT'},\n",
       "      'charspan': [0, 54]}]}],\n",
       "  'headings': ['1 Introduction'],\n",
       "  'origin': {'mimetype': 'application/pdf',\n",
       "   'binary_hash': 11465328351749295394,\n",
       "   'filename': '2408.09869v5.pdf'}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "loader = DoclingLoader(\n",
    "    file_path=FILE_PATH,\n",
    "    export_type=EXPORT_TYPE,\n",
    "    chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
    ")\n",
    "\n",
    "docs_hybrid = loader.load()\n",
    "for dh in docs_hybrid[:3]:\n",
    "    print(f\"- {d.page_content=}\")\n",
    "dh.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aafe214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh == d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06194c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres import PGVector\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"my_docs\",\n",
    "    connection=\"postgresql+psycopg://...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204aa7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated PDF saved as: annotated_2012-lyme-legislature.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import psycopg\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def annotate_pdf(psql_conn, source_filename, output_dir = None, output_filename=None):\n",
    "    \"\"\"\n",
    "    Given a PostgreSQL connection and a source filename (as stored in the \"source\" key of the cmetadata column),\n",
    "    this function queries the database for all rows associated with that document and then adds rectangle annotations\n",
    "    onto the PDF file for each chunk's bounding boxes. Each chunk (as identified by its chunk_index) is assigned a distinct color.\n",
    "    \n",
    "    The function converts bounding box coordinates from a bottom-left origin system (as stored in your JSON) to the\n",
    "    top-left origin system used by PyMuPDF.\n",
    "    \n",
    "    Parameters:\n",
    "      psql_conn: an open psycopg2 connection to your PostgreSQL database.\n",
    "      source_filename: the file path (or source value) identifying your document.\n",
    "      output_filename: optional; if provided, the annotated PDF will be saved using this filename. \n",
    "                       Otherwise, \"_annotated\" is appended to the original filename.\n",
    "    \"\"\"\n",
    "    # Query the database for rows where the JSONB column \"cmetadata\" has the given source.\n",
    "    query = \"\"\"\n",
    "        SELECT cmetadata\n",
    "        FROM public.langchain_pg_embedding\n",
    "        WHERE cmetadata->>'source' = %s\n",
    "        ORDER BY (cmetadata->>'chunk_index')::int;\n",
    "    \"\"\"\n",
    "    with psql_conn.cursor() as cur:\n",
    "        cur.execute(query, (source_filename,))\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No entries found for source:\", source_filename)\n",
    "        return\n",
    "\n",
    "    # Open the PDF document using PyMuPDF.\n",
    "    doc = fitz.open(source_filename)\n",
    "\n",
    "    # Define a palette of colors as RGB tuples with each component in [0, 1].\n",
    "    colors = [\n",
    "        (1, 0, 0),   # red\n",
    "        (0, 1, 0),   # green\n",
    "        (0, 0, 1),   # blue\n",
    "        (1, 1, 0),   # yellow\n",
    "        (1, 0, 1),   # magenta\n",
    "        (0, 1, 1),   # cyan\n",
    "        # You can add additional colors if needed.\n",
    "    ]\n",
    "\n",
    "    # Loop over each row in the query result.\n",
    "    for row in rows:\n",
    "        # Each row contains the cmetadata JSON. It might already be a dictionary; if not, parse it.\n",
    "        cmetadata = row[0]\n",
    "        if isinstance(cmetadata, str):\n",
    "            cmetadata = json.loads(cmetadata)\n",
    "        \n",
    "        # Retrieve the chunk index to select the annotation color.\n",
    "        chunk_index = cmetadata.get(\"chunk_index\")\n",
    "        if chunk_index is None:\n",
    "            continue\n",
    "\n",
    "        # Select a color by cycling through the palette.\n",
    "        color = colors[chunk_index % len(colors)]\n",
    "\n",
    "        # Retrieve the document-level metadata and its associated items.\n",
    "        dl_meta = cmetadata.get(\"dl_meta\", {})\n",
    "        doc_items = dl_meta.get(\"doc_items\", [])\n",
    "        for item in doc_items:\n",
    "            # Each document item should contain a list under the key \"prov\".\n",
    "            prov_list = item.get(\"prov\", [])\n",
    "            for prov in prov_list:\n",
    "                # Get the bounding box dictionary.\n",
    "                bbox = prov.get(\"bbox\")\n",
    "                if not bbox:\n",
    "                    continue\n",
    "                \n",
    "                # Retrieve the page number (assumed 1-indexed in your metadata).\n",
    "                page_no = prov.get(\"page_no\")\n",
    "                if not page_no or page_no < 1 or page_no > doc.page_count:\n",
    "                    continue\n",
    "                page = doc.load_page(page_no - 1)  # PyMuPDF uses 0-indexed pages.\n",
    "                \n",
    "                # Extract horizontal coordinates.\n",
    "                left = bbox.get(\"l\")\n",
    "                right = bbox.get(\"r\")\n",
    "                # Extract vertical coordinates from the JSON (using bottom-left origin).\n",
    "                orig_bottom = bbox.get(\"b\")\n",
    "                orig_top = bbox.get(\"t\")\n",
    "                if None in (left, right, orig_bottom, orig_top):\n",
    "                    continue\n",
    "\n",
    "                # Determine if the bbox uses bottom-left coordinates.\n",
    "                coord_origin = bbox.get(\"coord_origin\", \"TOPLEFT\")  # Default assume top-left if not provided.\n",
    "                if coord_origin.upper() == \"BOTTOMLEFT\":\n",
    "                    # For PyMuPDF (top-left system), transform the y-coordinates.\n",
    "                    page_height = page.rect.height\n",
    "                    # Invert the y-coordinates.\n",
    "                    new_top = page_height - orig_top\n",
    "                    new_bottom = page_height - orig_bottom\n",
    "                else:\n",
    "                    # If already in top-left coordinate system, use as-is.\n",
    "                    new_top = orig_top\n",
    "                    new_bottom = orig_bottom\n",
    "\n",
    "                # Create a rectangle. In PyMuPDF, fitz.Rect(left, top, right, bottom).\n",
    "                rect = fitz.Rect(left, new_top, right, new_bottom)\n",
    "\n",
    "                page.draw_rect(\n",
    "                    rect,\n",
    "                    color=color,      # Border color (can be None if no border)\n",
    "                    fill=color,       # Fill color\n",
    "                    fill_opacity=0.2, # Transparency\n",
    "                    width=1.0,              # Border width (optional)\n",
    "                    overlay=True\n",
    "                )\n",
    "                # # Add a rectangle annotation to the page.\n",
    "                # annot = page.addRectAnnot(rect)\n",
    "                # annot.setColors(stroke=color)   # Set the annotation's border (stroke) color.\n",
    "                # annot.setBorder(width=1)          # Set the border width if desired.\n",
    "                # annot.update()\n",
    "\n",
    "    # Define an output filename if not provided.\n",
    "    if not output_filename:\n",
    "        output_filename = 'annotated_' + os.path.basename(source_filename)\n",
    "        \n",
    "    output_filepath = os.path.join(output_dir, output_filename) if output_dir else output_filename\n",
    "    # Save the annotated PDF.\n",
    "    doc.save(output_filepath, garbage=4, deflate=True)\n",
    "    doc.close()\n",
    "    print(\"Annotated PDF saved as:\", output_filename)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Connect to your PostgreSQL database. Adjust connection parameters as needed.\n",
    "    conn = psycopg.connect(\n",
    "        dbname=os.getenv('DB_NAME', 'rag_lyme_docs'),\n",
    "        user=os.getenv('DB_USER', 'postgres'),\n",
    "        password=os.getenv('DB_PASSWORD', 'your_password'),\n",
    "        host=os.getenv('DB_HOST', 'localhost'),\n",
    "    )\n",
    "    \n",
    "    # Specify the PDF file (as stored in the JSON metadata \"source\").\n",
    "    source_pdf = \"test_rag_documents/2012-lyme-legislature.pdf\"\n",
    "\n",
    "    output_dir = os.getenv('ANNOTATED_DOCUMENT_DIRECTORY', None)\n",
    "    \n",
    "    # Call the function to create an annotated duplicate of the PDF.\n",
    "    annotate_pdf(conn, source_pdf, output_dir = output_dir)\n",
    "    \n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eeb740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
